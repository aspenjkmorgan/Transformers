{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment 5: Transformers and Natural Language Processing (Part 2, V2)**\n",
        "# **Note: there is a separate submission portal for part 2 on Moodle**\n",
        "\n",
        "## *YOUR FULL NAME HERE*\n",
        "Netid: Your netid here\n",
        "\n",
        "Note: this assignment falls under collaboration Mode 2: Individual Assignment â€“ Collaboration Permitted. Please refer to the syllabus for additional information.\n"
      ],
      "metadata": {
        "id": "N1CRJy7ib-pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 3: Text Classification with A Large Language Model  (30 points)**  In this example you will utilize a modern large language model to classify text.  Specifically, you will use load the pre-trained BERT encoder that we discussed in class, and then fine-tune it to solve a custom text classification problem where you classify news articles into one of four categories: world, sports, business, sci/tech.   \n",
        "\n",
        "To assist with this exercise, we will need to make use of some libraries from Hugging Face, an organization that provides many widely-used libraries to support deep learning applications ([link](https://huggingface.co/)).   \n",
        "\n",
        "Below is a code skeleton for completing this task, with comments to guide you through the process of completing it. Please complete the code below and submit a pdf of your completed code with results.  *There will be a separate submission portal for this question on Moodle.  Although your code will be reviewed, you will be graded primarily based graded upon the correctness of your output*  \n",
        "\n",
        "Although the code skeleton below provides useful guidance/hints to fill in teh code, I highly recommend that you review a tutorial on text classification provided by hugging face before, or while, you complete this exercise ([tutorial link](https://huggingface.co/docs/transformers/en/tasks/sequence_classification))\n"
      ],
      "metadata": {
        "id": "PKR2vc7Tcaw1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installations:** Make sure you use pip or conda to install the following\n",
        "libraries for this exercise:  datasets, evaluate, metrics, transformers, numpy, and torch.\n",
        "\n",
        "Google Colab already has torch and numpy, but you will still need to install\n",
        "transformers, datasets, evaluate and metrics.  You can copy and paste the line below into colab and it will install them.\n",
        "\n",
        "*pip install transformers datasets evaluate accelerate*"
      ],
      "metadata": {
        "id": "WxJfoQbSnvNm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igpB1LqdbM7B"
      },
      "outputs": [],
      "source": [
        "# Necessary Imports\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Load the AG News dataset using load_dataset\n",
        "dataset = load_dataset(\"ag_news\")\n",
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "\n",
        "#Load the tokenizer for a BERT-based model \"TinyBERT\", and specify the number of labels\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\",num_labels=4)\n",
        "\n",
        "# Define a function to tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# TODO: Tokenize the training and testing data. Hint: use .map to apply the tokenize function above to your train and test datasets\n",
        "train_dataset = ...\n",
        "test_dataset = ...\n",
        "\n",
        "# Load TinyBERT model We use TinyBERT, which requires substantially less\n",
        "# compute than BERT, with only a modest reduction in accuracy\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"huawei-noah/TinyBERT_General_4L_312D\", num_labels=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Rc6DZWDbM7D"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',             # output directory\n",
        "    num_train_epochs=3,                 # number of training epochs\n",
        "    per_device_train_batch_size=8,      # batch size for training\n",
        "    per_device_eval_batch_size=16,      # batch size for evaluation\n",
        "    warmup_steps=500,                   # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,                  # strength of weight decay\n",
        "    logging_dir='./logs',               # directory for storing logs\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmkhVn02bM7D"
      },
      "outputs": [],
      "source": [
        "# TODO: Function to compute accuracy of the model\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = ...\n",
        "    return {'accuracy': (predictions == labels).mean()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVPqBGHwbM7E"
      },
      "outputs": [],
      "source": [
        "# TODO: Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=...,\n",
        "    args=...,\n",
        "    train_dataset=...,\n",
        "    eval_dataset=...,\n",
        "    compute_metrics=...\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# TODO: Evaluate the model\n",
        "results = ...\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFHqv9cPbM7E"
      },
      "outputs": [],
      "source": [
        "num_examples = 6\n",
        "\n",
        "def get_example(data, idx):\n",
        "    return data['text'][idx], data['label'][idx]\n",
        "\n",
        "# TODO: Make a label mapping dictionary for the AG News dataset (keys should be numbers and values should be the category as a string)\n",
        "label_map = {...}\n",
        "\n",
        "# TODO: Select num_examples examples from the test dataset\n",
        "examples_text = []\n",
        "examples_label = []\n",
        "for i in range(num_examples):\n",
        "    text, label = get_example(test_dataset, i)\n",
        "    ...\n",
        "    ...\n",
        "\n",
        "# TODO: Tokenize the examples\n",
        "# Hint: similar to how we defined the tokenize_function above, except here you also want to set return_tensors=\"pt\"\n",
        "# to ensure that the output from the tokenizer is ready for a PyTorch model\n",
        "inputs = [tokenizer(...) for text in examples_text]\n",
        "\n",
        "# Move to the same device as model\n",
        "if torch.cuda.is_available():\n",
        "    inputs = [{k: v.cuda() for k, v in inp.items()} for inp in inputs]\n",
        "    model.cuda()\n",
        "\n",
        "# For people with a GPU on a Macintosh machine, uncomment this\n",
        "# elif torch.backends.mps.is_available():\n",
        "#     inputs = [input.to(device) for input in inputs]\n",
        "#     device = torch.device(\"mps\")\n",
        "#     model = model.to(device)\n",
        "\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    outputs = [model(**inp) for inp in inputs]\n",
        "\n",
        "# TODO: Extract logits from the output and apply softmax to get probabilities\n",
        "# Hint: ModelOutput class documentation https://huggingface.co/docs/transformers/en/main_classes/output\n",
        "probabilities = [... for output in outputs]\n",
        "\n",
        "# Get the predicted class indices\n",
        "predicted_classes = [torch.argmax(prob, dim=-1) for prob in probabilities]\n",
        "\n",
        "# TODO: Print 6 examples where you have the example text on one line, and the true and predicted labels on the next.\n",
        "for i in range(num_examples):\n",
        "    ...\n",
        "    ...\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}